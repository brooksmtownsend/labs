[
{
	"uri": "https://criticalstack.github.io/labs/install/awsaccount/",
	"title": "AWS Account Setup",
	"tags": [],
	"description": "",
	"content": " AWS Account Creation  Go to AWS and register for a new free account.\n Sign up for a new account:\n Fill our your information including Payment Information so that Amazon can verify your identity.\n Verify your account:\n Sign in to your account:\n  Manage User Access  From the AWS Management Console -\u0026gt; Find Services search for IAM (Manage User Access and Encryption Key).\n Click Policies from the Navigation.\n It is recommended that you create a new limited access policy rather than giving a user admin access. For the purpose of the Critical Stack install we will create a new policy with only the permissions needed to build the cluster. Click Create policy .\n In the new Create policy window you can create a new policy with the Visual editor or with JSON (JavaScript Object Notation). Select JSON\n You can use use this sample json policy definition or copy the text below within the JSON editor window.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;CriticalStackInstallationPolicy\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:AttachInstances\u0026quot;, \u0026quot;autoscaling:AttachLoadBalancers\u0026quot;, \u0026quot;autoscaling:AttachLoadBalancerTargetGroups\u0026quot;, \u0026quot;autoscaling:CreateAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:CreateLaunchConfiguration\u0026quot;, \u0026quot;autoscaling:CreateOrUpdateTags\u0026quot;, \u0026quot;autoscaling:DeleteAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:DeleteLaunchConfiguration\u0026quot;, \u0026quot;autoscaling:DeleteTags\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingNotificationTypes\u0026quot;, \u0026quot;autoscaling:DescribeLaunchConfigurations\u0026quot;, \u0026quot;autoscaling:DescribeLoadBalancers\u0026quot;, \u0026quot;autoscaling:DescribeLoadBalancerTargetGroups\u0026quot;, \u0026quot;autoscaling:DescribePolicies\u0026quot;, \u0026quot;autoscaling:DescribeScalingActivities\u0026quot;, \u0026quot;autoscaling:DescribeTags\u0026quot;, \u0026quot;autoscaling:DetachInstances\u0026quot;, \u0026quot;autoscaling:DetachLoadBalancers\u0026quot;, \u0026quot;autoscaling:DetachLoadBalancerTargetGroups\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:SetInstanceHealth\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:UpdateAutoScalingGroup\u0026quot;, \u0026quot;ec2:AllocateAddress\u0026quot;, \u0026quot;ec2:AssociateAddress\u0026quot;, \u0026quot;ec2:AssociateIamInstanceProfile\u0026quot;, \u0026quot;ec2:AssociateRouteTable\u0026quot;, \u0026quot;ec2:AttachInternetGateway\u0026quot;, \u0026quot;ec2:AttachNetworkInterface\u0026quot;, \u0026quot;ec2:AttachVolume\u0026quot;, \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:CopyImage\u0026quot;, \u0026quot;ec2:CreateInternetGateway\u0026quot;, \u0026quot;ec2:CreateKeyPair\u0026quot;, \u0026quot;ec2:CreateNatGateway\u0026quot;, \u0026quot;ec2:CreateNetworkAcl\u0026quot;, \u0026quot;ec2:CreateNetworkInterface\u0026quot;, \u0026quot;ec2:CreateRoute\u0026quot;, \u0026quot;ec2:CreateRouteTable\u0026quot;, \u0026quot;ec2:CreateSecurityGroup\u0026quot;, \u0026quot;ec2:CreateSnapshot\u0026quot;, \u0026quot;ec2:CreateSubnet\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:CreateVolume\u0026quot;, \u0026quot;ec2:CreateVpc\u0026quot;, \u0026quot;ec2:DeleteInternetGateway\u0026quot;, \u0026quot;ec2:DeleteKeyPair\u0026quot;, \u0026quot;ec2:DeleteNatGateway\u0026quot;, \u0026quot;ec2:DeleteNetworkAcl\u0026quot;, \u0026quot;ec2:DeleteNetworkInterface\u0026quot;, \u0026quot;ec2:DeleteRoute\u0026quot;, \u0026quot;ec2:DeleteRouteTable\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot;, \u0026quot;ec2:DeleteSnapshot\u0026quot;, \u0026quot;ec2:DeleteSubnet\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot;, \u0026quot;ec2:DeleteVolume\u0026quot;, \u0026quot;ec2:DeleteVpc\u0026quot;, \u0026quot;ec2:DescribeAccountAttributes\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:DescribeFpgaImages\u0026quot;, \u0026quot;ec2:DescribeImageAttribute\u0026quot;, \u0026quot;ec2:DescribeImages\u0026quot;, \u0026quot;ec2:DescribeInstanceAttribute\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeInstanceStatus\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;ec2:DescribeKeyPairs\u0026quot;, \u0026quot;ec2:DescribeNatGateways\u0026quot;, \u0026quot;ec2:DescribeNetworkAcls\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaceAttribute\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaces\u0026quot;, \u0026quot;ec2:DescribeRouteTables\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;ec2:DescribeSnapshots\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DescribeVolumes\u0026quot;, \u0026quot;ec2:DescribeVolumeStatus\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:DetachInternetGateway\u0026quot;, \u0026quot;ec2:DetachNetworkInterface\u0026quot;, \u0026quot;ec2:DetachVolume\u0026quot;, \u0026quot;ec2:DisassociateAddress\u0026quot;, \u0026quot;ec2:DisassociateIamInstanceProfile\u0026quot;, \u0026quot;ec2:DisassociateRouteTable\u0026quot;, \u0026quot;ec2:DisassociateSubnetCidrBlock\u0026quot;, \u0026quot;ec2:ModifyFpgaImageAttribute\u0026quot;, \u0026quot;ec2:ModifyImageAttribute\u0026quot;, \u0026quot;ec2:ModifyInstanceAttribute\u0026quot;, \u0026quot;ec2:ModifyNetworkInterfaceAttribute\u0026quot;, \u0026quot;ec2:ModifySubnetAttribute\u0026quot;, \u0026quot;ec2:ModifyVpcAttribute\u0026quot;, \u0026quot;ec2:MonitorInstances\u0026quot;, \u0026quot;ec2:ReleaseAddress\u0026quot;, \u0026quot;ec2:ReportInstanceStatus\u0026quot;, \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;ec2:StartInstances\u0026quot;, \u0026quot;ec2:StopInstances\u0026quot;, \u0026quot;ec2:TerminateInstances\u0026quot;, \u0026quot;elasticloadbalancing:AddTags\u0026quot;, \u0026quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:AttachLoadBalancerToSubnets\u0026quot;, \u0026quot;elasticloadbalancing:ConfigureHealthCheck\u0026quot;, \u0026quot;elasticloadbalancing:CreateAppCookieStickinessPolicy\u0026quot;, \u0026quot;elasticloadbalancing:CreateLBCookieStickinessPolicy\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancerListeners\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancerPolicy\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancerListeners\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancerPolicy\u0026quot;, \u0026quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:DescribeInstanceHealth\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerPolicies\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerPolicyTypes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTags\u0026quot;, \u0026quot;elasticloadbalancing:DetachLoadBalancerFromSubnets\u0026quot;, \u0026quot;elasticloadbalancing:DisableAvailabilityZonesForLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:EnableAvailabilityZonesForLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:RemoveTags\u0026quot;, \u0026quot;elasticloadbalancing:SetLoadBalancerListenerSSLCertificate\u0026quot;, \u0026quot;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\u0026quot;, \u0026quot;elasticloadbalancing:SetLoadBalancerPoliciesOfListener\u0026quot;, \u0026quot;iam:AddRoleToInstanceProfile\u0026quot;, \u0026quot;iam:AttachRolePolicy\u0026quot;, \u0026quot;iam:CreateInstanceProfile\u0026quot;, \u0026quot;iam:CreatePolicy\u0026quot;, \u0026quot;iam:CreatePolicyVersion\u0026quot;, \u0026quot;iam:CreateRole\u0026quot;, \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:DeleteInstanceProfile\u0026quot;, \u0026quot;iam:DeletePolicy\u0026quot;, \u0026quot;iam:DeletePolicyVersion\u0026quot;, \u0026quot;iam:DeleteRole\u0026quot;, \u0026quot;iam:DeleteRolePolicy\u0026quot;, \u0026quot;iam:DeleteServiceLinkedRole\u0026quot;, \u0026quot;iam:DetachRolePolicy\u0026quot;, \u0026quot;iam:GetInstanceProfile\u0026quot;, \u0026quot;iam:GetPolicy\u0026quot;, \u0026quot;iam:GetPolicyVersion\u0026quot;, \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:GetRolePolicy\u0026quot;, \u0026quot;iam:ListAttachedRolePolicies\u0026quot;, \u0026quot;iam:ListEntitiesForPolicy\u0026quot;, \u0026quot;iam:ListInstanceProfiles\u0026quot;, \u0026quot;iam:ListInstanceProfilesForRole\u0026quot;, \u0026quot;iam:ListPolicies\u0026quot;, \u0026quot;iam:ListPolicyVersions\u0026quot;, \u0026quot;iam:ListRolePolicies\u0026quot;, \u0026quot;iam:ListRoles\u0026quot;, \u0026quot;iam:PassRole\u0026quot;, \u0026quot;iam:RemoveRoleFromInstanceProfile\u0026quot;, \u0026quot;s3:*\u0026quot;, \u0026quot;sts:GetCallerIdentity\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Your policy should look something like this:\n Select Review Policy.\n Give your policy a name and description and select Create policy.\n Click Users from the Navigation and Add user.\n Select Programmatic access for the Access Type and Click on Next: Permission at the bottom of the page.\n Create a new group for permissions by selecting Create group.\n Click Refresh and search for the policy you just created CS_Install_Policy. Click the checkbox by the policy name and give your group a name. Select Create Group.\n With group selected click Next: Tags from the Add user screen.\n Add any optional tags and select Next: Review.\n Select Create user to finalize to user creation.\n The Access key ID and Secret access key will be used for the CS Installer.\n  "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/node/deploystateless/",
	"title": "Deploying a Stateless App",
	"tags": [],
	"description": "",
	"content": " Deploying a Stateless Node App in Critical Stack Getting Started Pre-requisites:\n Node JS (and npm) : Node.JS Docker : Docker Public container registry (Docker Hub is easiest, Artifactory works too) : Docker Hub  Overview In this lab we will create a simple NodeJS application, deploy it via Critical Stack. and access it via a public URL.\nSteps Building  Open a terminal window. In your current working directory (we use the Development directory under the user\u0026rsquo;s home directory in this example), create a lab directory called node-lab and a subdirectory of that called app:\nuser@testhost Development$ mkdir -p node-lab/app user@testhost Development$ cd node-lab user@testhost node-lab$ ls app  Using the editor of your choice, create a file called index.js inside the app directory (resulting in app/index.js) with the following content:\n// Say hello from Node var http = require('http'); http.createServer(function (req, res) { res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello from Node!\\n'); }).listen(3000, \u0026quot;0.0.0.0\u0026quot;); console.log('Server running on :3000');  Build the app configuration with npm in your node-lab working directory (not the app directory, this is one level above app)\nnpm init\nAccept all defaults except package name, which should be something like hello-node, version number (this is definitely a 0.0.1 release!), and entry point, as that will be app/index.js\nuser@testhost node-lab$ npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help json` for definitive documentation on these fields and exactly what they do. Use `npm install \u0026lt;pkg\u0026gt;` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quite. package name: (user) hello-node version: (1.0.0) 0.0.1 description: entry point: (index.js) app/index.js test command: git repository: keywords: author: license: (ISC) About to write to /Users/user/Development/node-lab/package.json:  npm makes it easy for JavaScript developers to share and re-use code. Install dependent packages shared by other developers by running this command:\nnpm install\nuser@testhost node-lab$ npm install npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN hello-node@0.0.1 No description npm WARN hello-node@0.0.1 No repository field. up to date in 10.179s found 0 vulnerabilities  Create a Dockerfile to target node and copy the necessary files into the Docker image (Note to instructor: explain base image from which this image is derived). Expose the desired TCP port where the app will listen.\nCreate a new file in your node-lab directory (not the app directory) called Dockerfile and paste in the content below.\nFROM node:9 # Make base directory RUN mkdir /src WORKDIR /src COPY ./package.json /src/package.json COPY ./package-lock.json /src/package-lock.json RUN npm install --silent COPY ./app /src/app EXPOSE 3000 CMD [\u0026quot;node\u0026quot;, \u0026quot;app/index.js\u0026quot;]  Build a Docker image using the Dockerfile with the tag hello-node\ndocker build -t hello-node -f Dockerfile .\nuser@testhost node-lab$ docker build -t hello-node -f Dockerfile . Sending build context to Docker daemon 71.68kB Step 1/8 : FROM node:9 ---\u0026gt; 08a8c8089ab1 Step 2/8 : RUN mkdir /src ...  Optional: Run the Docker image as a container instance locally to test that the app behaves as expected:\ndocker run -p 3000:3000 --rm -ti hello-node\nuser@testhost node-lab$ docker run -p 3000:3000 --rm -ti hello-node Server running on :3000  Optional: Verify app works in a new terminal window:\ncurl http://localhost:3000/\nuser@testhost ~$ curl http://localhost:3000/ Hello from Node!  Note: to stop the running container run this command from the new terminal window (this uses --filter (-f) to find the container created from your hello-node image):\ndocker stop $(docker ps -qf \u0026quot;ancestor=hello-node\u0026quot;)\nuser@testhost ~$ docker stop $(docker ps -qf \u0026quot;ancestor=hello-node\u0026quot;) 42c18d268c56  You should see the container exit and your shell prompt return in your original terminal window.\n Login to your public container registry (in this example we use Docker Hub). Note your user name as you will use this later.\ndocker login\nuser@testhost node-lab$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: \u0026lt;your username here\u0026gt; Password: Login Succeeded  Add the tag to be used in the public container registry (Note to instructor: Advise on tag naming convention)\ndocker tag hello-node \u0026lt;your-user-name\u0026gt;/hello-node:0.0.1\nuser@testhost node-lab$ docker tag hello-node \u0026lt;your-user-name\u0026gt;/hello-node:0.0.1  Push of the tagged image into the public container registry\ndocker push \u0026lt;your-user-name\u0026gt;/hello-node:0.0.1\nuser@testhost node-lab$ docker tag hello-node \u0026lt;your-user-name\u0026gt;/hello-node:0.0.1 The push refers to repository [docker.io/\u0026lt;your-user-name\u0026gt;/hello-node] 99599c00434b: Pushed 2c18bd33cb20: Pushed 41b63aaeffc4: Pushed df64ff0ed93f: Pushed 71521673e105: Mounted from library/node 7695686f75c0: Mounted from library/node e492023cc4f9: Mounted from library/node cbda574aa37a: Mounted from library/node 8451f9fe0016: Mounted from library/node 858cd8541f7e: Mounted from library/node a42d312a03bb: Mounted from library/node dd1eb1fd7e08: Mounted from library/node 0.0.1: digest: sha256:e755e97b58a700207b2a9ba0deaa26927210b84f8d79618d1c4cd8f498b97373 size: 2834  Your image digest and layer hashes will differ.\n  Deploying  Login to Critical Stack. Under Data Center \u0026gt; Workloads select Deployments. Create a Simple Deployment in Critical Stack.\nCall the app name whatever you like (in this example I used my-first-deployment).\nPoint to the tagged Docker image recently pushed into the image registry (docker.io/\u0026lt;your-user-name\u0026gt;/hello-node:0.0.1)\nLeave the rest of the inputs at their defaults\n Under Data Center \u0026gt; Workloads select Deployments and confirm that the deployment created in the previous step is Available (a non-zero number is visible in the Available column).\n Under Data Center \u0026gt; Services and Discovery select Services. Create a Simple Service in Critical Stack.\nCall the service name whatever you like (in this example I used my-first-deployment-svc).\nMatch the Selector name to the app/pod name which was deployed in the previous step.\nSelect Load Balancer as the option for Mode\nCall the listener name whatever you like but something unique relative to the service name (in this example I used my-first-deployment-listener).\nSelect TCP as the protocol\nUnder the Port field, enter the desired port which will be used by the Load Balancer connecting the app to the \u0026ldquo;live\u0026rdquo; public internet (typically port 80).\nUnder the Target Port field, enter the port number supported by your app and exposed in your Dockerfile (in this example, port 3000)\n Under Data Center \u0026gt; Services and Discovery select Services. Check the service listing and note the dynamic port which was assigned to it. In this example it is port 32151\n Under Data Center \u0026gt; Services and Discovery select Endpoints. Confirm that the service created has the just created listener configured to the port supported by the app and exposed in the Dockerfile (in this example, port 3000).\n Log into the cloud provider console associated with this deployment of Critical Stack (note: we are running our Critical Stack cluster on AWS EC2. The steps which follow will be AWS-specific).\n Navigate to the Load Balancers section\n Find the Load Balancer which was created by Critical Stack/Kubernetes during the creation of the Service in a previous step (step 12). To quickly find the relevant Load Balancer, enter kubernetes.io/service-name as the Tag Key in the Filter text box.\nFor the Tag Value, enter in the name of the service created in the previous step. In this example, our service name is \u0026lsquo;critical-stack/my-first-deployment-svc`.\n Click the Load Balancer found in the previous step. At the bottom of the screen you should see a tab called Description. Within the data displayed under that tab you should see DNS name under the Basic section. Copy this DNS value as it is the auto-generated, publicly facing domain name.\n Paste the DNS value copied from a previous step into a browser. This confirms that the app we have deployed onto our Critical Stack cluster is fully \u0026ldquo;live\u0026rdquo; to the public internet.\n  Conclusion We created a simple NodeJS application, packaged the application in a Docker image, pushed the Docker image to a public Docker Hub repository, pulled that Docker image into a Critical Stack deployment as a container instance, and accessed the application via a public URL.\nTo learn the basics of managing the lifecycle of an application, see the next lab.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/go/hello/",
	"title": "Deploying a Stateless Go App",
	"tags": [],
	"description": "",
	"content": " Deploying a Stateless Golang App in Critical Stack Getting Started Pre-requisites:\n Go Visual Studio Code curl or some equivalent way to do GET and POST If using curl, python to make JSON prettier Docker : Docker Public container registry Docker Hub A Critical Stack deployment with a user account provisioned for you. Note your namespace when you login.  Overview In this lab, you will deploy a simple Go Hello World application in Critical Stack and create Services to make it available externally. We will then illustrate how upgrade the deployment by creating a new release of the application.\nLifted from Making a RESTful JSON API in Go and adapted to Critical Stack, this is just example code and not intended to teach proper Go coding in any form.\nBuilding your Hello World App  Create a working directory to build your Hello World application. Open a terminal window. In your current working directory (we will use the Development directory under the user\u0026rsquo;s home directory in this example), create a lab directory called go and a subdirectory of that call \u0026lsquo;app\u0026rsquo;\ncd ~/Development mkdir -p go/app cd go/app  Using the editor of your choice, create a new file called main.go inside the app directory.\nvi hello-go.go  Add the following content to your new file and save:\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;html\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;Hello, %q\u0026quot;, html.EscapeString(r.URL.Path)) }) log.Fatal(http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil)) }  Compile an executable (in this example a GO executable) which properly targets a linux OS and is built statically with bare-minimum libs and dependencies. In this example we have decided to name the executable hello-go.\n$ CGO_ENABLED=0 GOARCH=amd64 GOOS=linux go build -ldflags=\u0026quot;-s -w\u0026quot; -a -o hello-go .  A new file called hello-go will be created in the same directory. This will be referenced when we build our docker container.\n Create a new file in the same directory and name it Dockerfile. This file could be named anything but for now we will just keep it simple. Copy the following code into this file:\n# STEP 1 build directory / file layout FROM ubuntu:latest as layout RUN groupadd -g 1000 appuser RUN useradd -r -u 1000 -g appuser appuser RUN mkdir -p /app/data \u0026amp;\u0026amp; chmod -R 755 /app # STEP 2 debug if needed FROM busybox:latest as builder # STEP 3 build a small, non-root runtime image FROM scratch COPY --from=layout /etc/passwd /etc/passwd COPY --from=layout /etc/group /etc/group COPY --chown=appuser:appuser --from=layout /app/data /app/data # For debug COPY --from=builder /bin/busybox /bin/busybox COPY --from=builder /bin/sh /bin/sh WORKDIR /app USER appuser # STEP 4 add application ADD hello-go /app/hello-go EXPOSE 8080 CMD [\u0026quot;/app/hello-go\u0026quot;]  Login to Docker registry\ndocker login\n$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: jabbottc1 Password: Login Succeeded  Build a Docker image using the Dockerfile. Make sure to include the . at the end of the following command. hello-go is the name of the image we are creating.\ndocker build -t hello-go -f Dockerfile .\n  Testing your Hello World App  Optional Step. If you want to test your new docker container locally and see if it behaves as expected you can run the following command:\ndocker run -e PORT=8080 -p 8080:8080 --rm -ti hello-go\n Verify the app works by using the following command in a new terminal window. Alternatively, you could open a browser to http://localhost:8080\ncurl http://localhost:8080\nNote: to stop the running container run this command from the new terminal window (this uses --filter (-f) to find the container created from your hello-go image):\ndocker stop $(docker ps -qf \u0026quot;ancestor=hello-go\u0026quot;)\nAnother way to stop the hello-gocontainer is to view all of the running docker containers and issue a command to stop it by the id\n$ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6009be3dfae3 hello-go \u0026quot;/hello-go\u0026quot; 5 seconds ago Up 4 seconds 0.0.0.0:8080-\u0026gt;8080/tcp goofy_swanson  $ docker stop 6009be3dfae3 6009be3dfae3 $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   Tagging and pushing your image to a container registry  Now that we have an image we need to apply a tag and push it to a container registry.\n Docker Tags allow you to convey useful information about a specific image version/variant. Rather than referring to your image by the IMAGE ID you can create an alias to your images. Lets look at the list of images locally (your list might be different):\ndocker images\n$ docker images REPOSITORY TAG IMAGE ID CREATED hello-go latest 56f2fefe6685 2 hours ago alpine latest 8cb3aa00f899 3 weeks ago tomcat 8.0 ef6a7c98d192 6 months ago  We will tag your local image with your namespace/repo\ndocker tag hello-go \u0026lt;your-registry-user-name\u0026gt;/hello-go:0.0.1\n$ docker tag hello-go jabbottc1/hello-go:0.0.1  Optional Step list your images and tags\ndocker images\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE jabbottc1/hello-go 0.0.1 511503f3b991 16 minutes ago 7.06MB hello-go latest 511503f3b991 16 minutes ago 7.06MB  Push the tagged image into your public container registry\ndocker push jabbottc1/hello-go:0.0.1\n$ docker push jabbottc1/hello-go:0.0.1 The push refers to repository [docker.io/jabbottc1/hello-go] 1c82c58a49f6: Layer already exists d992cd40d944: Layer already exists e4ae77eef13a: Layer already exists 8e3f00a45858: Layer already exists 5483fef4153b: Layer already exists c1b4e3786f95: Layer already exists 0.0.1: digest: sha256:385984109a3cf6b1a82dcebb1ee124a7172a5a81795bf8116f2cab6d7a09ca4e size: 1569  Your image digest and layer hashes will differ.\n  Deploy your container into Critical Stack  Follow deployment steps similar to Node lab with appropriate substitutions.\n Navigate to your application host to see your Hello World message or run curl to verify that you application is running and exposed externally. Note it may take a few minutes for this new cname to be created.\ncurl -s http://\u0026lt;URL_for_your_deployment\u0026gt;\n$ curl http://... Hello, \u0026quot;/\u0026quot;$   Conclusion You have successfully deployed a golang application in Critical Stack. But Hello World isn\u0026rsquo;t enough, right? Next we will deploy a golang application with a REST API.\nTODO Show how to build with scripts 1. sh build.sh -v 0.0.1\n"
},
{
	"uri": "https://criticalstack.github.io/labs/faqs/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " What is Critical Stack? Critical Stack is the secure container orchestration platform for the enterprise. Critical Stack allows you to enforce security, compliance, and regulatory controls for your business while optimizing the efficiency, scalability, resilience of your environment. The Critical Stack orchestration platform is the secure, automated and simple way to manage your infrastructure and deploy containerized workloads on which your company depends.\nWhat Is Critical Stack\u0026rsquo;s Relationship With Kubernetes? While Kubernetes is a scalable, resilient, and low-latency container orchestration platform, it does not offer an easy-to-use interface or security and compliance configurations that highly regulated enterprises expect. In response to this need, Critical Stack built a robust set of features that enable enterprises to more easily adopt and secure containerized infrastructure at scale.\nCritical Stack does not fork the Kubernetes source code and we do not do anything proprietary to YAML-based configurations used in Kubernetes. Critical Stack starts with Kubernetes and add our internal standards around regression testing, security, or performance.\nHow much does Critical Stack cost? Developers can utilize Critical Stack for free, up to 5 nodes (cloud resources) within a Critical Stack cluster per month. Should an environment require more than 5 nodes within a cluster, you can add your credit card information via the Account Management page to upgrade your license to Premium, which charges $5/month for each additional node spun up within that month. Your monthly bill will be calculated based on the total number of new nodes spun up in a calendar month, not the number of nodes active at any given time. For example, if a customer with 5 nodes in February spins up 2 additional nodes in March and then deactivated 1 prior to the end of the billing cycle, they will be charged $10 at the end of March.\nPlease note that AWS costs for any nodes spun up within the Critical Stack product are not included in Critical Stack’s pricing/billing. It is your responsibility to pay your cloud provider directly.\nIn the future, Critical Stack plans to offer an enhanced, enterprise tier of its software that addresses more complex environments. More information will be released on that offering at a later date.\nWhat Are Critical Stack\u0026rsquo;s Most Distinguishing Features And Architecture? Opinionated Choices About Kubernetes Critical Stack’s opinionated implementation of Kubernetes is designed to increase security, improve performance, and make Kubernetes more user-friendly.\nIn every Critical Stack cluster, we configure the kernel to reflect our opinions on security and performance down to the lowest levels of the cluster. An example of an opinionated choice related to security is that each Critical Stack cluster is configured with Security-Enhanced Linux (SELinux) to support granular access control security policies.\nAs another example, Critical Stack chooses to use a Container Network Interface (CNI) called Cilium, which offers performance enhancements through the use of Berkeley Packet Filtering (BPF). In addition to performance, Cilium also gives us security features such as the ability to configure Network Security Policies at the layer 7 protocol level.\nSecurity Features Critical Stack includes the following features:\n Executing processes can be secured through Critical Stack using container aware SELinux Policies. Intra-namespace and Inter-namespace communications can be secured with orchestration aware layer 3 and 4 policies. Layer 7 protocol analyzers allow the enforcement of granular API security. Admission controllers can be used to ensure executing services comply with all required organizational policy. The Critical Stack API and User Interface (UI) use JSON Web Tokens for authentication and authorization (a popular authentication mechanism for web interchange).  In the future, real-time dynamic risk scoring will allow organizations to measure and react to a dynamic risk environment.\nUser Interface The Critical Stack User Interface allows for immediate monitoring of the nodes and the containers and pods within in the cluster. Creating new components within the cluster is very easy with the simple wizards, and the advanced editors that provide yaml editing and formatting.\nRESTful API While the UI is easy to use, the API allows for automation to be built into your CI/CD pipeline. Every functionality the UI provides is exposed through the API.\nMarketplace Besides keeping your container cluster secure, another prominent feature Critical Stack offers is its Marketplace. The Critical Stack Marketplace provides a public repository that houses published, pre-configured stacks (app specs) that Critical Stack users can replicate/install into their clusters.\nIn the future, the Marketplace will allow Critical Stack customers to submit their own app specs to the Critical Stack Marketplace for review, vetting, and (if accepted) publication into the public Marketplace. Additionally, Critical Stack will soon support companies publishing to their own private marketplaces, enabling developers within their organization to share app specs across clusters and teams.\nInstaller Critical Stack has a click-through installer that enables installation of Critical Stack into a new VPC within your AWS account in a matter of minutes.\nHow Does Critical Stack Compare To Other Alternatives? When comparing containerization to non-containerized workloads,containerization reduces the attack surface, while also providing security, portability, and efficiency.\nWhen comparing Critical Stack to other container orchestration solutions, Critical Stack offers a container orchestration solution that is “by the enterprise, for the enterprise”. Critical Stack is engineered for complex, regulated environments (like Capital One) with a focus on providing a container orchestration solution that can truly address all the jobs to be done at the enterprise level.\nLastly, for those who are interested in or using serverless computing, while a serverless architecture may work for many workloads, most serverless solutions available in the industry today are not as portable as containerized solutions. Given containerization (and Critical Stack) do not rigidly lock in customers to a specific architecture or product, customers can get all of the benefits of containerization (and most of the benefits of serverless) without handcuffing themselves to the solution.\nDoes/How-Does Critical Stack Integrate With An Aspect Of My Existing Solution? Critical Stack is enterprise software, not SaaS. This means that Critical Stack installs into your AWS cloud environment. Once the Critical Stack software is installed, you will have the ability to manage your infrastructure and deploy workloads within the software.\nAs an example, within Capital One’s enterprise environment, Critical Stack has integrated with (or is in the process of integrating with) standard patterns and practices such as (but not limited to):\n Building Containerized Microservices Transforming Monoliths into Containerized Microservices CI/CD Pipelines Metrics Collection/Aggregation Log Aggregation Monitoring Audit Logging Open Policy Agent (OPA) High Availability Autoscaling Vault/Secrets Integration Single Sign On (SSO)  The Critical Stack team is working to publish share-able content for these Patterns \u0026amp; Practices on Critical Stack.\nHow Does Critical Stack Support And Integrate With The Cloud? Critical Stack is installable within all US Amazon Web Services (AWS) regions, with all standard functionality and cloud resources available to workloads running on Critical Stack.\nAs part of our future roadmap, we plan to make Critical Stack compatible with other cloud providers such as Azure, GCP and Digital Ocean.\nCan I Use Critical Stack On-Premises? Critical Stack does not have an on-prem solution available today. As part of our future roadmap, we will look at producing an on-prem solution. Please submit any on-premise feature requests and suggestions to support@criticalstack.com.\nI\u0026rsquo;m Not That Familiar with Containers Or Container Orchestration Where Can I Go To Learn More A Beginner-Friendly Introduction to Containers, VMs, and Docker\nHow to Choose the Right Container Orchestration and How to Deploy It\nA Kubernetes FAQ for the C-Suite\nLearn Kubernetes in Under 3 Hours: A Detailed Guide to Orchestrating Containers\n"
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/",
	"title": "Feature Labs",
	"tags": [],
	"description": "",
	"content": " Feature Labs Labs that explore specific features of the Critical Stack product\n"
},
{
	"uri": "https://criticalstack.github.io/labs/install/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": " Installation Labs Labs that get your foundation set up so that you can continue on to the feature labs and other advanced topics.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/node/",
	"title": "NodeJS Labs",
	"tags": [],
	"description": "",
	"content": "  Feature Labs Feature labs illustrated using NodeJS\n"
},
{
	"uri": "https://criticalstack.github.io/labs/install/aws_basic_cluster/",
	"title": "AWS Basic Cluster",
	"tags": [],
	"description": "",
	"content": " Installing Critical Stack into an AWS account This document provides guidance with the installation of Critical Stack into an AWS account.\nPrerequisites  An AWS account An AWS Access key ID and Secret access key Instructions for setting up a new account and access keys - see AWS Account Setup  Account Setup  Register for a new account at portal.criticalstack.com.\n Select Register from the link at the bottom of the page.\n Read the Terms of Service and if you agree click Accept.\n Select a registration type. If the cloud service provider (CSP) account into which Critical Stack will be installed is owned by and paid for by a registered business, click the \u0026ldquo;Business\u0026rdquo; registration type. Otherwise, select \u0026ldquo;Individual\u0026rdquo;.\n After you fill our your user information click Register, you will receive a Welcome Email and a link to confirm your email.\n Login to your account at Critical Stack Developer Portal. In the Key Generator section, click Generate License button to produce a valid license key. Select Cloud as the License Type since we are installing into AWS.\n After generating a license key, click the Go to Installer button at the top right of the page to run the Critical Stack installer in your AWS environment. By logging into the installer, the system will automatically link your account’s generated license key to your Critical Stack instance.\n  Installation  There are two types of installations: Cloud and Local. For this walkthrough, we will be installing in AWS so select Cloud and your license key and click Proceed.\n Select your Cloud provider, AWS\n Enter your AWS Credentials to create and configure the required resources. It is strongly recommended you create a limited access role. You can use this sample json definition to create a policy for your user/group. Additional information for creating IAM Policies and generating keys can be found here\n Select your AWS Region for this cluster and click Proceed.\n You may add any optional Cluster Tags and click Proceed.\n If you want to customize the network settings you can add your own VPC CIDR and Subnets or use the default. Click on Proceed. Additional details on VPCs and Subnets can be found here\n Choose what Instance Type you want for your Master Node. For detailed comparison ECS instance types this guide is very helpful.\n Choose Public for the subnet type unless you are connecting to a private subnet with a VPN.\n Choose what type of Storage you want to use, either gp2 General Purpose SSD volume (balance of price and performance) or io1 Highest-Performance SSD volume (low-latency / high throughput workloads) and the storage size. Select Proceed.\n Review the cluster settings. If you need to make any changes you can use the Go Back button or click on a section header in the sidebar to make the desired changes. Note The access_key_id and secret_access_key will not be stored and is only showed for you convenience to verify. If everything looks correct, click on Create Cluster  When you cluster is ready, select Download Assets to get the details about your environment.\n  Cluster Assets The Cluster Assets page lists the Cluster information, PEM Key, and and Cluster Link. If you plan to ssh into the cluster nodes, you should download they PEM Key. You can come back to this page at any time if you need to access the cluster information.\n To access the Critical Stack cluster click the Go To Cluster button or copy the url provided in the Cluster Link section.  TODO - Add login info after May 15 "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/go/",
	"title": "Go Labs",
	"tags": [],
	"description": "",
	"content": "  Feature Labs Feature labs illustrated using Go\n"
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/node/updating/",
	"title": "Updating an App",
	"tags": [],
	"description": "",
	"content": " Using Critical Stack to easily update a scalable, stateless application Getting Started Pre-requisites: Previous Node lab\nOverview In the previous lab we created a simple NodeJS application, packaged the application in a Docker image, pushed the Docker image to a public Docker Hub repository, pulled that Docker image into a Critical Stack deployment as a container instance, and accessed the application via a public URL.\nNow we will create more instances of the container, update the application to show bread crumbs of those instances, push that update to Docker Hub, and use the Critical Stack UI to instruct K8s to roll out the update while we refresh on the public URL to see the change deploy seamlessly.\nSteps  Edit app/index.js and replace the source code with the following:\n// Load the http module to create an http server. var http = require('http'); // Load the os module to access network interfaces. var os = require('os'); // Load the crypto module to generate a random number. var crypto = require(\u0026quot;crypto\u0026quot;); // Get all interfaces by IP address var interfaces = os.networkInterfaces(); var addresses = []; for (var k in interfaces) { for (var k2 in interfaces[k]) { var address = interfaces[k][k2]; if (address.family === 'IPv4' \u0026amp;\u0026amp; !address.internal) { addresses.push(address.address); } } } // Generate a random number var random_number = crypto.randomBytes(16).toString(\u0026quot;hex\u0026quot;); // Configure our HTTP server to respond with Hello World to all requests. var server = http.createServer(function (request, response) { response.writeHead(200, {\u0026quot;Content-Type\u0026quot;: \u0026quot;text/plain\u0026quot;}); response.end(\u0026quot;Hello World from \u0026quot; + addresses + \u0026quot; (\u0026quot; + random_number + \u0026quot;)\\n\u0026quot;); }); // Listen on port 3000 server.listen(3000, \u0026quot;0.0.0.0\u0026quot;); // Put a friendly message on the terminal console.log(\u0026quot;Server IPs: \u0026quot; + addresses); console.log(\u0026quot;Random number: \u0026quot; + random_number); console.log(\u0026quot;Server running at http://0.0.0.0:3000/\u0026quot;);  Increment the application version number in your package.json file and re-run npm install to update the package-lock.json file.\nuser@testhost node-lab$ cat package.json { \u0026quot;name\u0026quot;: \u0026quot;hello-node\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.0.2\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Say hello from Node\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;app/index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Error: no test specified\\\u0026quot; \u0026amp;\u0026amp; exit 1\u0026quot; }, \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot; } user@testhost node-lab$ npm install npm WARN hello-node@0.0.2 No repository field. up to date in 0.632s found 0 vulnerabilities  Build your Docker image again, but this time tag it as \u0026lt;your-username\u0026gt;/hello-node:0.0.2 so we can uniquely identify the new image. After it\u0026rsquo;s built, push it to DockerHub so we can update it in your Critical Stack cluster.\nuser@testhost node-lab$ docker build -t \u0026lt;your-username\u0026gt;/hello-node:0.0.2 -f Dockerfile . Sending build context to Docker daemon 4.288MB Step 1/9 : FROM node:9 ---\u0026gt; 08a8c8089ab1 Step 2/9 : RUN mkdir /src ---\u0026gt; Using cache ---\u0026gt; 3530d1bbe38a Step 3/9 : WORKDIR /src ---\u0026gt; Using cache ---\u0026gt; 4096063f6a2b Step 4/9 : COPY ./package.json /src/package.json ---\u0026gt; ad23e7eb360e Step 5/9 : COPY ./package-lock.json /src/package-lock.json ---\u0026gt; 46c675f287b6 Step 6/9 : RUN npm install --silent ---\u0026gt; Running in d3144afb3a54 up to date in 0.076s Removing intermediate container d3144afb3a54 ---\u0026gt; 56cbd547eb6d Step 7/9 : COPY ./app /src/app ---\u0026gt; 50b39bb7ea64 Step 8/9 : EXPOSE 3000 ---\u0026gt; Running in 0ab1df6c4e2c Removing intermediate container 0ab1df6c4e2c ---\u0026gt; 67b69d1d4337 Step 9/9 : CMD [\u0026quot;node\u0026quot;, \u0026quot;app/index.js\u0026quot;] ---\u0026gt; Running in 7925bde282fc Removing intermediate container 7925bde282fc ---\u0026gt; be1e663ce0d2 Successfully built be1e663ce0d2 Successfully tagged \u0026lt;your-username\u0026gt;/hello-node:0.0.2 user@testhost node-lab$ docker push \u0026lt;your-username\u0026gt;/hello-node:0.0.2 The push refers to repository [docker.io/\u0026lt;your-username\u0026gt;/hello-node] 13108b60c999: Pushed 328a2707105e: Pushed 700b0faff5ac: Pushed eb80df9a56cf: Pushed df64ff0ed93f: Layer already exists 71521673e105: Layer already exists 7695686f75c0: Layer already exists e492023cc4f9: Layer already exists cbda574aa37a: Layer already exists 8451f9fe0016: Layer already exists 858cd8541f7e: Layer already exists a42d312a03bb: Layer already exists dd1eb1fd7e08: Layer already exists 0.0.2: digest: sha256:2e1002b635cd983e7f2458331079f7b131d8096fd5994287c6f4933438513367 size: 3041  Go to Critical Stack UI under Deployment Listings, find the row my-first-deployment, click on the gear icon, and select Scale. Move the slider for Number of pods to 2 and then click OK.\n The previous step declaratively set the state of desired pods. Watch the deployment scale from 1 to 2.\n Click on the gear icon again, and select Edit. On line 36, change the Docker image tag from 0.0.1 to 0.0.2, for example: image: 'docker.io/jabbottc1/hello-node:0.0.1' to image: 'docker.io/jabbottc1/hello-node:0.0.2'. Then click Save and then Exit.\nNote that while we are editing the deployment, another way to scale the application is to directly update the yaml as shown below.\n Refresh the public URL to your application and watch the update roll out. Also notice the different output when load changes from one pod to another. Note: changing Idle timeout to 1 in the Load Balancer configuration will reduce the amount of time between switching from one pod to another (in the same section used to get the public facing URL in the previous lab).\n Want to go back to the previous application version? Edit the deployment again and change the Docker image tag from 0.0.2 back to 0.0.1.\n  Conclusion You should now feel comfortable with the basics of pushing new Docker images to a repository and pulling them into a Critical Stack deployment running multiple instances of your image. This process is the basis for ongoing updates.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Critical Stack Labs Self-driven lab activities for getting to know the Critical Stack product\n"
},
{
	"uri": "https://criticalstack.github.io/labs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://criticalstack.github.io/labs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]