[
{
	"uri": "https://criticalstack.github.io/labs/faqs/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " What is Critical Stack? Critical Stack is the secure container orchestration platform for the enterprise. Critical Stack allows you to enforce security, compliance, and regulatory controls for your business while optimizing the efficiency, scalability, resilience of your environment. The Critical Stack orchestration platform is the secure, automated and simple way to manage your infrastructure and deploy containerized workloads on which your company depends.\nWhat Is Critical Stack\u0026rsquo;s Relationship With Kubernetes? While Kubernetes is a scalable, resilient, and low-latency container orchestration platform, it does not offer an easy-to-use interface or security and compliance configurations that highly regulated enterprises expect. In response to this need, Critical Stack built a robust set of features that enable enterprises to more easily adopt and secure containerized infrastructure at scale.\nCritical Stack does not fork the Kubernetes source code and we do not do anything proprietary to YAML-based configurations used in Kubernetes. Critical Stack starts with Kubernetes and add our internal standards around regression testing, security, or performance.\nHow much does Critical Stack cost? Developers can utilize Critical Stack for free, up to 5 nodes (cloud resources) within a Critical Stack cluster per month. Should an environment require more than 5 nodes within a cluster, you can add your credit card information via the Account Management page to upgrade your license to Premium, which charges $5/month for each additional node spun up within that month. Your monthly bill will be calculated based on the total number of new nodes spun up in a calendar month, not the number of nodes active at any given time. For example, if a customer with 5 nodes in February spins up 2 additional nodes in March and then deactivated 1 prior to the end of the billing cycle, they will be charged $10 at the end of March.\nPlease note that AWS costs for any nodes spun up within the Critical Stack product are not included in Critical Stack’s pricing/billing. It is your responsibility to pay your cloud provider directly.\nIn the future, Critical Stack plans to offer an enhanced, enterprise tier of its software that addresses more complex environments. More information will be released on that offering at a later date.\nWhat Are Critical Stack\u0026rsquo;s Most Distinguishing Features And Architecture? Opinionated Choices About Kubernetes Critical Stack’s opinionated implementation of Kubernetes is designed to increase security, improve performance, and make Kubernetes more user-friendly.\nIn every Critical Stack cluster, we configure the kernel to reflect our opinions on security and performance down to the lowest levels of the cluster. An example of an opinionated choice related to security is that each Critical Stack cluster is configured with Security-Enhanced Linux (SELinux) to support granular access control security policies.\nAs another example, Critical Stack chooses to use a Container Network Interface (CNI) called Cilium, which offers performance enhancements through the use of Berkeley Packet Filtering (BPF). In addition to performance, Cilium also gives us security features such as the ability to configure Network Security Policies at the layer 7 protocol level.\nSecurity Features Critical Stack includes the following features:\n Executing processes can be secured through Critical Stack using container aware SELinux Policies. Intra-namespace and Inter-namespace communications can be secured with orchestration aware layer 3 and 4 policies. Layer 7 protocol analyzers allow the enforcement of granular API security. Admission controllers can be used to ensure executing services comply with all required organizational policy. The Critical Stack API and User Interface (UI) use JSON Web Tokens for authentication and authorization (a popular authentication mechanism for web interchange).  In the future, real-time dynamic risk scoring will allow organizations to measure and react to a dynamic risk environment.\nUser Interface The Critical Stack User Interface allows for immediate monitoring of the nodes and the containers and pods within in the cluster. Creating new components within the cluster is very easy with the simple wizards, and the advanced editors that provide yaml editing and formatting.\nRESTful API While the UI is easy to use, the API allows for automation to be built into your CI/CD pipeline. Every functionality the UI provides is exposed through the API.\nMarketplace Besides keeping your container cluster secure, another prominent feature Critical Stack offers is its Marketplace. The Critical Stack Marketplace provides a public repository that houses published, pre-configured stacks (app specs) that Critical Stack users can replicate/install into their clusters.\nIn the future, the Marketplace will allow Critical Stack customers to submit their own app specs to the Critical Stack Marketplace for review, vetting, and (if accepted) publication into the public Marketplace. Additionally, Critical Stack will soon support companies publishing to their own private marketplaces, enabling developers within their organization to share app specs across clusters and teams.\nInstaller Critical Stack has a click-through installer that enables installation of Critical Stack into a new VPC within your AWS account in a matter of minutes.\nHow Does Critical Stack Compare To Other Alternatives? When comparing containerization to non-containerized workloads,containerization reduces the attack surface, while also providing security, portability, and efficiency.\nWhen comparing Critical Stack to other container orchestration solutions, Critical Stack offers a container orchestration solution that is “by the enterprise, for the enterprise”. Critical Stack is engineered for complex, regulated environments (like Capital One) with a focus on providing a container orchestration solution that can truly address all the jobs to be done at the enterprise level.\nLastly, for those who are interested in or using serverless computing, while a serverless architecture may work for many workloads, most serverless solutions available in the industry today are not as portable as containerized solutions. Given containerization (and Critical Stack) do not rigidly lock in customers to a specific architecture or product, customers can get all of the benefits of containerization (and most of the benefits of serverless) without handcuffing themselves to the solution.\nDoes/How-Does Critical Stack Integrate With An Aspect Of My Existing Solution? Critical Stack is enterprise software, not SaaS. This means that Critical Stack installs into your AWS cloud environment. Once the Critical Stack software is installed, you will have the ability to manage your infrastructure and deploy workloads within the software.\nAs an example, within Capital One’s enterprise environment, Critical Stack has integrated with (or is in the process of integrating with) standard patterns and practices such as (but not limited to):\n Building Containerized Microservices Transforming Monoliths into Containerized Microservices CI/CD Pipelines Metrics Collection/Aggregation Log Aggregation Monitoring Audit Logging Open Policy Agent (OPA) High Availability Autoscaling Vault/Secrets Integration Single Sign On (SSO)  The Critical Stack team is working to publish share-able content for these Patterns \u0026amp; Practices on Critical Stack.\nHow Does Critical Stack Support And Integrate With The Cloud? Critical Stack is installable within all US Amazon Web Services (AWS) regions, with all standard functionality and cloud resources available to workloads running on Critical Stack.\nAs part of our future roadmap, we plan to make Critical Stack compatible with other cloud providers such as Azure, GCP and Digital Ocean.\nCan I Use Critical Stack On-Premises? Critical Stack does not have an on-prem solution available today. As part of our future roadmap, we will look at producing an on-prem solution. Please submit any on-premise feature requests and suggestions to support@criticalstack.com.\nI\u0026rsquo;m Not That Familiar with Containers Or Container Orchestration Where Can I Go To Learn More A Beginner-Friendly Introduction to Containers, VMs, and Docker\nHow to Choose the Right Container Orchestration and How to Deploy It\nA Kubernetes FAQ for the C-Suite\nLearn Kubernetes in Under 3 Hours: A Detailed Guide to Orchestrating Containers\n"
},
{
	"uri": "https://criticalstack.github.io/labs/install/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": " Installation Labs Labs that get your foundation set up so that you can continue on to the feature labs and other advanced topics.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/administration/ssh_master_node/",
	"title": "SSH to Master Nodes via Bastion Host",
	"tags": [],
	"description": "",
	"content": " Securely Connect to a Critical Stack Master Node running in a Private VPC Overview The Critical Stack master nodes are not located on a public subnet. If you need to access the nodes you will first need to create a bastion host. This guide will help explain how to securely access these environments without uploading private keys to the secondary nodes.\nPrerequisities  A Critical Stack cluster deployed using the installer in an external AWS account and the cluster private key (.pem) locally saved. You will need access to the AWS account where Critical Stack was installed and privilages to create EC2 instances and Security Groups These instructions assume you will be connecting from a Mac/Linux terminal. If you are connecting from a Windows client it is recommended you setup SSH key forwarding so that you do not have to copy the private key to a secondary host.\n  Getting Started Create a bastion host  Log in to your AWS Console.\n From the AWS Management Console, search for EC2\n Create an EC2 instance on AWS in the same account and datacenter as the CS deployment. From the AWS EC2 Console select Launch Instance.\n Choose t2.micro sized instance from the one of the Linux based AMIs since they are on the free-tier and you will only use this instance to connect to the master node.\n Select Configure Instance from the top navigation. For Network choose the Critical Stack VPC. For Subnet, select one of the public subnets. You can leave all of the other default values on this page.\n Select Configure Security Group from the top navigation.\n Create a new security group to allow SSH traffic over TCP on port 22. If you choose Anywhere, all IP addresses will have access to your instance (narrow this to an IP address range only you will be using for better security). Click on Review and Launch.\n Select Launch on the Review Instance Launch page\n Select Choose and existing key pair and  from the Select a key pair drop down list. This is the private key from the Critical Stack cluster. Acknowledge you have the key and select Launch.\n Select View Instances to to back to the AWS Console and view the EC2 Instances.\n Identify the Public IP of the bastion host and the Private IP of the master node.\n  Connect to your hosts  Open a terminal window.\n Configure SSH Agent Forwarding by adding the Critical Stack cluster private key to your local SSH agent. This will allow you to pass the cluster private key to the master node after you are connected to the bastion host (thereby eliminating the need to copy the private key to your bastion host).\nssh-add -K \u0026lt;clusterprivatekey\u0026gt;   SSH to your bastion host using the -A flag to to forward the key.\nssh -A -i ~/.ssh/\u0026lt;your_cluster_private_key\u0026gt;.pem ec2-user@\u0026lt;bastion_host_public_IP\u0026gt;\n From the bastion host, ssh into the master node. Identify the master node IP from the AWS EC2 Console. The master nodes will be prefixed with CS-Cluster- (case significant). Note worker nodes will be all lower case, cs-cluster-. Since we are passing the cluster private key through SSH forwarding you will only need to know the Critical Stack user csos.\n ssh csos@\u0026lt;master_node_private_IP\u0026gt;\n  "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/",
	"title": "Feature Labs",
	"tags": [],
	"description": "",
	"content": " Feature Labs Labs that explore specific features of the Critical Stack product\n"
},
{
	"uri": "https://criticalstack.github.io/labs/administration/",
	"title": "Administration",
	"tags": [],
	"description": "",
	"content": " Administration Labs for administering a Critical Stack cluster\n"
},
{
	"uri": "https://criticalstack.github.io/labs/administration/helm_support/",
	"title": "Helm Support for Critical Stack",
	"tags": [],
	"description": "",
	"content": " Helm Chart Support for Critical Stack Getting Started Pre-requisites:\n A Critical Stack cluster\n Master node with access to the Internet for helm, helm charts, and Docker images\n  Overview This document describes how to deploy helm to support deployment of helm chart-based applications.\nNote that the next version of helm will not include tiller so steps to deploy will change significantly (expected to be much simpler and more secure).\nSteps  SSH to master node\n Get helm\ncurl -f https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; /tmp/get_helm.sh sudo sh /tmp/get_helm.sh sudo helm init\t# --tiller-tls-verify  Fix RBACs\nsudo kubectl create serviceaccount --namespace kube-system tiller sudo kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller sudo kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  Sync number of replicas (1:1 to number of master nodes). Re-run this step if you change the number of master nodes.\nsudo kubectl -n kube-system scale deployment/tiller-deploy --replicas=$(kubectl get nodes --selector='node-role.kubernetes.io/master' | grep -v '^NAME ' | wc -l)  Deploy helm charts, e.g., jenkins\nsudo helm install stable/jenkins --namespace development   Follow-up  Enable --tiller-tls-verify for better security  Troubleshooting tips  To uninstall helm client and tiller server, first try:\nsudo helm reset --force  Delete tiller server deployment\nsudo kubectl -n kube-system delete deployment tiller-deploy  Delete tiller server replicaset (old style tiller)\nsudo kubectl -n kube-system delete replicaset tiller-deploy- # use \u0026lt;ESC\u0026gt; to complete  Delete tiller server pod\nsudo kubectl -n kube-system delete pod tiller-deploy- # use \u0026lt;ESC\u0026gt; to complete  Uninstall helm chart, delete pods, delete helm release\nsudo helm delete \u0026lt;chart\u0026gt; --purge   "
},
{
	"uri": "https://criticalstack.github.io/labs/install/awsaccount/",
	"title": "AWS Account Setup",
	"tags": [],
	"description": "",
	"content": " AWS Account Creation  Go to AWS and register for a new free account.\n Sign up for a new account:\n Fill our your information including Payment Information so that Amazon can verify your identity.\n Verify your account:\n Sign in to your account:\n  Manage User Access  From the AWS Management Console -\u0026gt; Find Services search for IAM (Manage User Access and Encryption Key).\n Click Policies from the Navigation.\n It is recommended that you create a new limited access policy rather than giving a user admin access. For the purpose of the Critical Stack install we will create a new policy with only the permissions needed to build the cluster. Click Create policy .\n In the new Create policy window you can create a new policy with the Visual editor or with JSON (JavaScript Object Notation). Select JSON\n You can use use this sample json policy definition or copy the text below within the JSON editor window.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;CriticalStackInstallationPolicy\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:AttachInstances\u0026quot;, \u0026quot;autoscaling:AttachLoadBalancers\u0026quot;, \u0026quot;autoscaling:AttachLoadBalancerTargetGroups\u0026quot;, \u0026quot;autoscaling:CreateAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:CreateLaunchConfiguration\u0026quot;, \u0026quot;autoscaling:CreateOrUpdateTags\u0026quot;, \u0026quot;autoscaling:DeleteAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:DeleteLaunchConfiguration\u0026quot;, \u0026quot;autoscaling:DeleteTags\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingNotificationTypes\u0026quot;, \u0026quot;autoscaling:DescribeLaunchConfigurations\u0026quot;, \u0026quot;autoscaling:DescribeLoadBalancers\u0026quot;, \u0026quot;autoscaling:DescribeLoadBalancerTargetGroups\u0026quot;, \u0026quot;autoscaling:DescribePolicies\u0026quot;, \u0026quot;autoscaling:DescribeScalingActivities\u0026quot;, \u0026quot;autoscaling:DescribeTags\u0026quot;, \u0026quot;autoscaling:DetachInstances\u0026quot;, \u0026quot;autoscaling:DetachLoadBalancers\u0026quot;, \u0026quot;autoscaling:DetachLoadBalancerTargetGroups\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:SetInstanceHealth\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:UpdateAutoScalingGroup\u0026quot;, \u0026quot;ec2:AllocateAddress\u0026quot;, \u0026quot;ec2:AssociateAddress\u0026quot;, \u0026quot;ec2:AssociateIamInstanceProfile\u0026quot;, \u0026quot;ec2:AssociateRouteTable\u0026quot;, \u0026quot;ec2:AttachInternetGateway\u0026quot;, \u0026quot;ec2:AttachNetworkInterface\u0026quot;, \u0026quot;ec2:AttachVolume\u0026quot;, \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:CopyImage\u0026quot;, \u0026quot;ec2:CreateInternetGateway\u0026quot;, \u0026quot;ec2:CreateKeyPair\u0026quot;, \u0026quot;ec2:CreateNatGateway\u0026quot;, \u0026quot;ec2:CreateNetworkAcl\u0026quot;, \u0026quot;ec2:CreateNetworkInterface\u0026quot;, \u0026quot;ec2:CreateRoute\u0026quot;, \u0026quot;ec2:CreateRouteTable\u0026quot;, \u0026quot;ec2:CreateSecurityGroup\u0026quot;, \u0026quot;ec2:CreateSnapshot\u0026quot;, \u0026quot;ec2:CreateSubnet\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:CreateVolume\u0026quot;, \u0026quot;ec2:CreateVpc\u0026quot;, \u0026quot;ec2:DeleteInternetGateway\u0026quot;, \u0026quot;ec2:DeleteKeyPair\u0026quot;, \u0026quot;ec2:DeleteNatGateway\u0026quot;, \u0026quot;ec2:DeleteNetworkAcl\u0026quot;, \u0026quot;ec2:DeleteNetworkInterface\u0026quot;, \u0026quot;ec2:DeleteRoute\u0026quot;, \u0026quot;ec2:DeleteRouteTable\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot;, \u0026quot;ec2:DeleteSnapshot\u0026quot;, \u0026quot;ec2:DeleteSubnet\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot;, \u0026quot;ec2:DeleteVolume\u0026quot;, \u0026quot;ec2:DeleteVpc\u0026quot;, \u0026quot;ec2:DescribeAccountAttributes\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:DescribeFpgaImages\u0026quot;, \u0026quot;ec2:DescribeImageAttribute\u0026quot;, \u0026quot;ec2:DescribeImages\u0026quot;, \u0026quot;ec2:DescribeInstanceAttribute\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeInstanceStatus\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;ec2:DescribeKeyPairs\u0026quot;, \u0026quot;ec2:DescribeNatGateways\u0026quot;, \u0026quot;ec2:DescribeNetworkAcls\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaceAttribute\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaces\u0026quot;, \u0026quot;ec2:DescribeRouteTables\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;ec2:DescribeSnapshots\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DescribeVolumes\u0026quot;, \u0026quot;ec2:DescribeVolumeStatus\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:DetachInternetGateway\u0026quot;, \u0026quot;ec2:DetachNetworkInterface\u0026quot;, \u0026quot;ec2:DetachVolume\u0026quot;, \u0026quot;ec2:DisassociateAddress\u0026quot;, \u0026quot;ec2:DisassociateIamInstanceProfile\u0026quot;, \u0026quot;ec2:DisassociateRouteTable\u0026quot;, \u0026quot;ec2:DisassociateSubnetCidrBlock\u0026quot;, \u0026quot;ec2:ModifyFpgaImageAttribute\u0026quot;, \u0026quot;ec2:ModifyImageAttribute\u0026quot;, \u0026quot;ec2:ModifyInstanceAttribute\u0026quot;, \u0026quot;ec2:ModifyNetworkInterfaceAttribute\u0026quot;, \u0026quot;ec2:ModifySubnetAttribute\u0026quot;, \u0026quot;ec2:ModifyVpcAttribute\u0026quot;, \u0026quot;ec2:MonitorInstances\u0026quot;, \u0026quot;ec2:ReleaseAddress\u0026quot;, \u0026quot;ec2:ReportInstanceStatus\u0026quot;, \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;ec2:StartInstances\u0026quot;, \u0026quot;ec2:StopInstances\u0026quot;, \u0026quot;ec2:TerminateInstances\u0026quot;, \u0026quot;elasticloadbalancing:AddTags\u0026quot;, \u0026quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:AttachLoadBalancerToSubnets\u0026quot;, \u0026quot;elasticloadbalancing:ConfigureHealthCheck\u0026quot;, \u0026quot;elasticloadbalancing:CreateAppCookieStickinessPolicy\u0026quot;, \u0026quot;elasticloadbalancing:CreateLBCookieStickinessPolicy\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancerListeners\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancerPolicy\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancerListeners\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancerPolicy\u0026quot;, \u0026quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:DescribeInstanceHealth\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerPolicies\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerPolicyTypes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTags\u0026quot;, \u0026quot;elasticloadbalancing:DetachLoadBalancerFromSubnets\u0026quot;, \u0026quot;elasticloadbalancing:DisableAvailabilityZonesForLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:EnableAvailabilityZonesForLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:RemoveTags\u0026quot;, \u0026quot;elasticloadbalancing:SetLoadBalancerListenerSSLCertificate\u0026quot;, \u0026quot;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\u0026quot;, \u0026quot;elasticloadbalancing:SetLoadBalancerPoliciesOfListener\u0026quot;, \u0026quot;iam:AddRoleToInstanceProfile\u0026quot;, \u0026quot;iam:AttachRolePolicy\u0026quot;, \u0026quot;iam:CreateInstanceProfile\u0026quot;, \u0026quot;iam:CreatePolicy\u0026quot;, \u0026quot;iam:CreatePolicyVersion\u0026quot;, \u0026quot;iam:CreateRole\u0026quot;, \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:DeleteInstanceProfile\u0026quot;, \u0026quot;iam:DeletePolicy\u0026quot;, \u0026quot;iam:DeletePolicyVersion\u0026quot;, \u0026quot;iam:DeleteRole\u0026quot;, \u0026quot;iam:DeleteRolePolicy\u0026quot;, \u0026quot;iam:DeleteServiceLinkedRole\u0026quot;, \u0026quot;iam:DetachRolePolicy\u0026quot;, \u0026quot;iam:GetInstanceProfile\u0026quot;, \u0026quot;iam:GetPolicy\u0026quot;, \u0026quot;iam:GetPolicyVersion\u0026quot;, \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:GetRolePolicy\u0026quot;, \u0026quot;iam:ListAttachedRolePolicies\u0026quot;, \u0026quot;iam:ListEntitiesForPolicy\u0026quot;, \u0026quot;iam:ListInstanceProfiles\u0026quot;, \u0026quot;iam:ListInstanceProfilesForRole\u0026quot;, \u0026quot;iam:ListPolicies\u0026quot;, \u0026quot;iam:ListPolicyVersions\u0026quot;, \u0026quot;iam:ListRolePolicies\u0026quot;, \u0026quot;iam:ListRoles\u0026quot;, \u0026quot;iam:PassRole\u0026quot;, \u0026quot;iam:RemoveRoleFromInstanceProfile\u0026quot;, \u0026quot;s3:*\u0026quot;, \u0026quot;sts:GetCallerIdentity\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Your policy should look something like this:\n Select Review Policy.\n Give your policy a name and description and select Create policy.\n Click Users from the Navigation and Add user.\n Select Programmatic access for the Access Type and Click on Next: Permission at the bottom of the page.\n Create a new group for permissions by selecting Create group.\n Click Refresh and search for the policy you just created CS_Install_Policy. Click the checkbox by the policy name and give your group a name. Select Create Group.\n With group selected click Next: Tags from the Add user screen.\n Add any optional tags and select Next: Review.\n Select Create user to finalize to user creation.\n The Access key ID and Secret access key will be used for the CS Installer.\n  "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/go/hello/",
	"title": "Deploying a Go App",
	"tags": [],
	"description": "",
	"content": " Deploying a Stateless Golang App in Critical Stack Getting Started Before starting this lab, you will need:\n Go installed Visual Studio Code (or your favorite IDE) curl or some equivalent way to do GET and POST \u0026nbsp; (The Postman browser plugin is a good alternative). If using curl, python to make JSON prettier (you can also use jq) Docker installed Access to a public container registry, e.g. Docker Hub A Critical Stack deployment with a user account provisioned for you. Make sure to take note of your namespace when you log in.  Overview In this lab, you will deploy a simple Go \u0026ldquo;Hello World\u0026rdquo; application to Critical Stack and create Services to make it accessible externally. We will then illustrate how upgrade the deployment by creating a new release of the application.\nThe Go code for this lab was lifted from Making a RESTful JSON API in Go and adapted to Critical Stack. This lab is not designed to illustrate Go programming practices or idioms.\nBuilding your Hello World App We need something to deploy, so let\u0026rsquo;s get started by creating the \u0026ldquo;Hello World\u0026rdquo; application.\nNote: all source materials for this lab can be found in the Critical Stack Feature Lab repository.\n Create a working directory to build your \u0026ldquo;Hello World\u0026rdquo; application.\n Open a terminal window.\n In your current working directory (we will use the Development directory under the user\u0026rsquo;s home directory in this example), create a lab directory called go and a subdirectory of that called app  cd ~/Development mkdir -p go/app cd go/app  Using the editor of your choice, create a new file called main.go inside the app directory.\nvi hello-go.go  Add the following content to your new file and save:\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;html\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;Hello, %q\u0026quot;, html.EscapeString(r.URL.Path)) }) log.Fatal(http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil)) }  Compile your Go executable so it properly targets a linux OS and is built statically with minimal dependencies. In this example we have decided to name the executable hello-go:\n$ CGO_ENABLED=0 GOARCH=amd64 GOOS=linux go build -ldflags=\u0026quot;-s -w\u0026quot; -a -o hello-go .  A new executable binary called hello-go will be created in the same directory. This will be referenced when we build our docker container.\n Create a new file in the same directory and name it Dockerfile. This file could be named anything, but common convention is to use this filename. Copy the following code into this file:\n# STEP 1 build directory / file layout FROM ubuntu:latest as layout RUN groupadd -g 1000 appuser RUN useradd -r -u 1000 -g appuser appuser RUN mkdir -p /app/data \u0026amp;\u0026amp; chmod -R 755 /app # STEP 2 debug if needed FROM busybox:latest as builder # STEP 3 build a small, non-root runtime image FROM scratch COPY --from=layout /etc/passwd /etc/passwd COPY --from=layout /etc/group /etc/group COPY --chown=appuser:appuser --from=layout /app/data /app/data # For debug COPY --from=builder /bin/busybox /bin/busybox COPY --from=builder /bin/sh /bin/sh WORKDIR /app USER appuser # STEP 4 add application ADD hello-go /app/hello-go EXPOSE 8080 CMD [\u0026quot;/app/hello-go\u0026quot;]  Login to your docker registry:\n$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: jabbottc1 Password: Login Succeeded  Build a Docker image using the Dockerfile. Make sure to include the . at the end of the following command. The name of the image we\u0026rsquo;re creating is hello-go:\ndocker build -t hello-go -f Dockerfile .\n  Testing Your Application Optional - if you want to test your docker image locally to see if it behaves as expected.\n Run the following command:\ndocker run -e PORT=8080 -p 8080:8080 --rm -ti hello-go\n Verify the app works by using the following command in a new terminal window. Alternatively, you could open a browser to http://localhost:8080\n$ curl http://localhost:8080  Note: to stop the running container run this command from the new terminal window (this uses \u0026ndash;filter (-f) to find the container created from your hello-go image):\ndocker stop $(docker ps -qf \u0026quot;ancestor=hello-go\u0026quot;)\nAnother way to stop the hello-go container is to view all of the running docker containers and issue a command to stop it by the ID:\n$ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6009be3dfae3 hello-go \u0026quot;/hello-go\u0026quot; 5 seconds ago Up 4 seconds 0.0.0.0:8080-\u0026gt;8080/tcp goofy_swanson  $ docker stop 6009be3dfae3 6009be3dfae3 $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   Tagging and Pushing your Image to a Registry Now that we have an image we need to apply a tag and push it to a container registry.\n Docker tags allow you to convey useful information about a specific image version or variant. Rather than referring to your image by the IMAGE ID you can create aliases for your images. Lets look at the list of images locally (your list might be different):\n$ docker images REPOSITORY TAG IMAGE ID CREATED hello-go latest 56f2fefe6685 2 hours ago alpine latest 8cb3aa00f899 3 weeks ago tomcat 8.0 ef6a7c98d192 6 months ago  We will tag your local image with your namespace/repository using the following command: docker tag hello-go \u0026lt;your-registry-user-name\u0026gt;/hello-go:0.0.1\n$ docker tag hello-go jabbottc1/hello-go:0.0.1  Optional Step list your images and tags\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE jabbottc1/hello-go 0.0.1 511503f3b991 16 minutes ago 7.06MB hello-go latest 511503f3b991 16 minutes ago 7.06MB  Push the tagged image into your public container registry\n$ docker push jabbottc1/hello-go:0.0.1 The push refers to repository [docker.io/jabbottc1/hello-go] 1c82c58a49f6: Layer already exists d992cd40d944: Layer already exists e4ae77eef13a: Layer already exists 8e3f00a45858: Layer already exists 5483fef4153b: Layer already exists c1b4e3786f95: Layer already exists 0.0.1: digest: sha256:385984109a3cf6b1a82dcebb1ee124a7172a5a81795bf8116f2cab6d7a09ca4e size: 1569  Your image digest and layer hashes will differ from the above terminal output.\n  Deploy your container into Critical Stack Deploying your container is as simple as following the deployment steps in the Node lab, but change the docker image name and other parameters as appropriate.\n Navigate to the application Load Balancer URL to see your Hello World message or run curl to verify that you application is running and exposed externally. Note it may take a few minutes for the Load Balancer to be created.\ncurl -s http://\u0026lt;URL_for_your_deployment\u0026gt;\n$ curl http://... Hello, \u0026quot;/\u0026quot;$   Conclusion You have now successfully deployed a Go application in Critical Stack. But Hello World isn\u0026rsquo;t enough, right? Next we will deploy a Stateless RESTful Go App.\nTODO Show how to build with scripts\n sh build.sh -v 0.0.1  "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/node/deploystateless/",
	"title": "Deploying a NodeJS App",
	"tags": [],
	"description": "",
	"content": " Deploying a Stateless NodeJS App in Critical Stack This lab will walk you through the process of deploying a stateless NodeJS application to Critical Stack.\nGetting Started Before we get started, you\u0026rsquo;ll need:\n Node JS (and npm) installed : Node.JS Docker installed : Docker Access to a public container registry (Docker Hub is easiest, Artifactory works too) : Docker Hub  Overview In this lab we will create a simple NodeJS application, deploy it via Critical Stack. and access it via a public URL.\nSteps Building  Open a terminal window. In your current working directory (in the example below we use the Development directory under the user\u0026rsquo;s home directory) create a directory for this lab called node-lab and a subdirectory of that called app:\nuser@testhost Development$ mkdir -p node-lab/app user@testhost Development$ cd node-lab user@testhost node-lab$ ls app  Using the editor of your choice, create a file called index.js inside the app directory (resulting in app/index.js) with the following content:\n// Say hello from Node var http = require('http'); http.createServer(function (req, res) { res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello from Node!\\n'); }).listen(3000, \u0026quot;0.0.0.0\u0026quot;); console.log('Server running on :3000');  Generate the app configuration with npm in the root of your node-lab directory.\nAccept all defaults except package name, which should be something like hello-node, version number (this is definitely a 0.0.1 release!), and entry point, as that will be app/index.js\nuser@testhost node-lab$ npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help json` for definitive documentation on these fields and exactly what they do. Use `npm install \u0026lt;pkg\u0026gt;` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quite. package name: (user) hello-node version: (1.0.0) 0.0.1 description: entry point: (index.js) app/index.js test command: git repository: keywords: author: license: (ISC) About to write to /Users/user/Development/node-lab/package.json:  npm makes it easy for JavaScript developers to share and re-use code. Install the dependencies for this lab by running this command:\nuser@testhost node-lab$ npm install npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN hello-node@0.0.1 No description npm WARN hello-node@0.0.1 No repository field. up to date in 10.179s found 0 vulnerabilities  Create a Dockerfile to target node and copy the necessary files into the docker image (Note to instructor: explain the base image from which this image is derived). The port you choose to expose should be the same as the port on which your application server listens. Your Dockerfile should be in the root of your node-lab directory and have the following content:\nFROM node:9 # Make base directory RUN mkdir /src WORKDIR /src COPY ./package.json /src/package.json COPY ./package-lock.json /src/package-lock.json RUN npm install --silent COPY ./app /src/app EXPOSE 3000 CMD [\u0026quot;node\u0026quot;, \u0026quot;app/index.js\u0026quot;]  Build a Docker image using the Dockerfile with the tag hello-node:\nuser@testhost node-lab$ docker build -t hello-node -f Dockerfile . Sending build context to Docker daemon 71.68kB Step 1/8 : FROM node:9 ---\u0026gt; 08a8c8089ab1 Step 2/8 : RUN mkdir /src ...  If you want to test this out locally before deploying it, you can use docker to launch the process in the container image to verify that it behaves as expected:\nuser@testhost node-lab$ docker run -p 3000:3000 --rm -ti hello-node Server running on :3000  Once this is running, you can check that the application works by using curl in a new terminal window:\nuser@testhost ~$ curl http://localhost:3000/ Hello from Node!  Note: to stop the running container run this command from the new terminal window (this uses -f to find the container created from your hello-node image):\nuser@testhost ~$ docker stop $(docker ps -qf \u0026quot;ancestor=hello-node\u0026quot;) 42c18d268c56  You should see the container exit and your shell prompt return in your original terminal window.\n Login to your public container registry (in this example we use Docker Hub). Note your user name as you will use this later.\nuser@testhost node-lab$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: \u0026lt;your username here\u0026gt; Password: Login Succeeded  Add the tag to be used in the public container registry (Note to instructor: Advise on tag naming convention)\nuser@testhost node-lab$ docker tag hello-node \u0026lt;your-user-name\u0026gt;/hello-node:0.0.1  Push the tagged image into a public container registry\nuser@testhost node-lab$ docker tag hello-node \u0026lt;your-user-name\u0026gt;/hello-node:0.0.1 The push refers to repository [docker.io/\u0026lt;your-user-name\u0026gt;/hello-node] 99599c00434b: Pushed 2c18bd33cb20: Pushed 41b63aaeffc4: Pushed df64ff0ed93f: Pushed 71521673e105: Mounted from library/node 7695686f75c0: Mounted from library/node e492023cc4f9: Mounted from library/node cbda574aa37a: Mounted from library/node 8451f9fe0016: Mounted from library/node 858cd8541f7e: Mounted from library/node a42d312a03bb: Mounted from library/node dd1eb1fd7e08: Mounted from library/node 0.0.1: digest: sha256:e755e97b58a700207b2a9ba0deaa26927210b84f8d79618d1c4cd8f498b97373 size: 2834  Your image digest and layer hashes will differ.\n  Deploying  Login to Critical Stack. Under Data Center \u0026gt; Workloads select Deployments. Create a Simple Deployment in Critical Stack. Call the app name whatever you like (in this example I used my-first-deployment). Next, point to the tagged Docker image recently pushed into the image registry (docker.io/\u0026lt;your-user-name\u0026gt;/hello-node:0.0.1). Leave the rest of the inputs at their defaults:\n Under Data Center \u0026gt; Workloads select Deployments and confirm that the deployment created in the previous step is Available (a non-zero number is visible in the Available column).\n Under Data Center \u0026gt; Services and Discovery select Services. Create a Simple Service in Critical Stack.\n Call the service name whatever you like (in this example I used my-first-deployment-svc). Match the Selector name to the app/pod name which was deployed in the previous step. Select Load Balancer as the option for Mode Call the listener name whatever you like but something unique relative to the service name (in this example I used my-first-deployment-listener). Select TCP as the protocol Under the Port field, enter the desired port which will be used by the Load Balancer connecting the app to the \u0026ldquo;live\u0026rdquo; public internet (typically port 80). Under the Target Port field, enter the port number supported by your app and exposed in your Dockerfile (in this example, port 3000)   Under Data Center \u0026gt; Services and Discovery select Services, check the service listing and note the dynamic port which was assigned to it. In this example it is port 32151 (but your port number will likely be different).\n Under Data Center \u0026gt; Services and Discovery select Endpoints, confirm that the service created has the just created listener configured to the port supported by the app and exposed in the Dockerfile (in this example, port 3000).\n Log into the cloud provider console associated with this deployment of Critical Stack (note: we are running our Critical Stack cluster on AWS EC2. The following steps will be AWS-specific).\n Navigate to the Load Balancers section:\n Find the Load Balancer created by Critical Stack/Kubernetes during the creation of the Service in step 12. To quickly find the relevant load balancer, enter kubernetes.io/service-name as the Tag Key in the Filter text box.\nFor the Tag Value, enter in the name of the service created in the previous step. In this example, our service name is critical-stack/my-first-deployment-svc.\n Click the Load Balancer found in the previous step. At the bottom of the screen you should see a tab called Description. Within the data displayed under that tab you should see DNS name under the Basic section. Copy this DNS value as it is the auto-generated, publicly facing domain name.\n Paste the DNS value copied from the previous step into a browser. This confirms that the app we have deployed onto our Critical Stack cluster is fully live and available to the public internet.\n  Conclusion We created a simple NodeJS application, packaged the application in a Docker image, pushed the Docker image to a public Docker Hub repository, pulled that Docker image into a Critical Stack deployment as a container instance, and accessed the application via a public URL.\nTo learn the basics of managing the lifecycle of an application, see the next lab.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/node/",
	"title": "NodeJS Labs",
	"tags": [],
	"description": "",
	"content": "  Feature Labs Feature labs illustrated using NodeJS\n"
},
{
	"uri": "https://criticalstack.github.io/labs/install/aws_basic_cluster/",
	"title": "AWS Basic Cluster",
	"tags": [],
	"description": "",
	"content": " Installing Critical Stack into an AWS account This document provides guidance with the installation of Critical Stack into an AWS account.\nPrerequisites  An AWS account An AWS Access key ID and Secret access key Instructions for setting up a new account and access keys - see AWS Account Setup  Account Setup  Register for a new account at portal.criticalstack.com.\n Select Register from the link at the bottom of the page.\n Read the Terms of Service and if you agree click Accept.\n Select a registration type. If the cloud service provider (CSP) account into which Critical Stack will be installed is owned by and paid for by a registered business, click the \u0026ldquo;Business\u0026rdquo; registration type. Otherwise, select \u0026ldquo;Individual\u0026rdquo;.\n After you fill our your user information click Register, you will receive a Welcome Email and a link to confirm your email.\n Login to your account at Critical Stack Developer Portal. In the Key Generator section, click Generate License button to produce a valid license key. Select Cloud as the *License Type since we are installing into AWS.\n After generating a license key, click the Go to Installer button at the top right of the page to run the Critical Stack installer in your AWS environment. By logging into the installer, the system will automatically link your account’s generated license key to your Critical Stack instance.\n  Installation  There are two types of installations: Cloud and Local. For this walkthrough, we will be installing to AWS so select Cloud and your license key and click Proceed.\n Select your Cloud provider, AWS\n Enter your AWS Credentials to create and configure the required resources. It is strongly recommended you create a limited access role. You can use this sample JSON definition to create a policy for your user/group. Additional information for creating IAM Policies and generating keys can be found here\n Select your AWS Region for this cluster and click Proceed.\n You may add any optional Cluster Tags and click Proceed.\n If you want to customize the network settings you can add your own VPC CIDR and Subnets or use the default. Click on Proceed. Additional details on VPCs and Subnets can be found here.\n Choose what Instance Type you want for your master node. For detailed comparison ECS instance types this guide is very helpful.\n Choose Public for the subnet type unless you are connecting to a private subnet with a VPN.\n Choose what type of Storage you want to use, either gp2 General Purpose SSD volume (balance of price and performance) or io1 Highest-Performance SSD volume (low-latency / high throughput workloads) and the storage size. Select Proceed.\n Review the cluster settings. If you need to make any changes you can use the Go Back button or click on a section header in the sidebar to make the desired changes. Note the access_key_id and secret_access_key will not be stored and is only showed for you convenience to verify. If everything looks correct, click CREATE CLUSTER.  When you cluster is ready, select Download Assets to get the details about your environment.\n  Cluster Assets The Cluster Assets page lists the Cluster information, PEM Key, and and Cluster Link. If you plan to ssh into the cluster nodes, you must download the PEM Key. You can come back to this page at any time if you need to access the cluster information.\n Create a cluster administrator login by clicking on the CREATE ADMIN button. You must complete this step to access the cluster, and this step can be completed only one time so note the password you choose.  To access the Critical Stack cluster click the GO TO CLUSTER button or copy the url provided in the Cluster Link section.  The Secure Orchestration Platform for the Enterprise   "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/go/hello_rest/",
	"title": "Deploying a Stateless RESTful Go App",
	"tags": [],
	"description": "",
	"content": " Deploying a Stateless Golang App with REST API in Critical Stack Getting Started Before starting this lab, you will need:\n Go installed Visual Studio Code or your favorite IDE installed curl or some equivalent way to do GET and POST \u0026nbsp; (Postman is a good alternative) If using curl, python to make JSON prettier (you can also use jq) Docker installed. Access to a public container registry, e.g. Docker Hub Completed the Previous Go lab for the Docker registry and Load Balancer setup - this demo will just update the previous Docker image with new functionality  Overview In this lab, we will be enhancing the work done in the previous lab to introduce a RESTful API and walking through the steps necessary to build and deploy this to Critical Stack.\nThe Go code was lifted from Making a RESTful JSON API in Go and adapted to Critical Stack. This is purely sample code and not intended to be used as a reference for best practices or idioms.\nBuilding Your RESTful Hello World App  Clone the accompanying Critical Stack Feature Lab repo if you haven\u0026rsquo;t already and change to the hello_rest directory so you can access the source code:\ngit clone https://github.com/criticalstack/labs-code.git\ncd labs-code/go/hello_rest\n As with previous lab, compile the REST API example code into hello-go.\n$ CGO_ENABLED=0 GOARCH=amd64 GOOS=linux go build -ldflags=\u0026quot;-s -w\u0026quot; -a -o hello-go .  As with previous lab, build a Docker image using the Dockerfile.\ndocker build -t hello-go -f Dockerfile .\n  Testing your RESTful Hello World App Optional Step: If you want to test your new docker container locally and see if it behaves as expected you can follow these steps:\n Run the following command:\ndocker run -e PORT=8080 -p 8080:8080 --rm -ti hello-go\n Verify the app works by using the following command in a new terminal window. Alternatively, you could open a browser to http://localhost:8080\nBasic test: curl -s http://localhost:8080\n$ curl -s http://localhost:8080 Welcome!  Show entries pre-inserted: curl -s http://localhost:8080/todos | python -m json.tool\n$ curl -s http://localhost:8080/todos | python -m json.tool [ { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;Write presentation\u0026quot; }, { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 2, \u0026quot;name\u0026quot;: \u0026quot;Host meetup\u0026quot; } ]  Show entry 1: \u0026nbsp; curl -s http://localhost:8080/todos/1 | python -m json.tool\ncurl -s http://localhost:8080/todos/1 | python -m json.tool { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;Write presentation\u0026quot; }  Insert a new entry:\n$ curl -s -H \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;name\u0026quot;:\u0026quot;New Todo\u0026quot;}' http://localhost:8080/todos {\u0026quot;id\u0026quot;:3,\u0026quot;name\u0026quot;:\u0026quot;New Todo\u0026quot;,\u0026quot;completed\u0026quot;:false,\u0026quot;due\u0026quot;:\u0026quot;0001-01-01T00:00:00Z\u0026quot;}  Show entry 3:\n$ curl -s http://localhost:8080/todos/3 | python -m json.tool { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 3, \u0026quot;name\u0026quot;: \u0026quot;New Todo\u0026quot; }  To stop the running container you can execute: \u0026nbsp; docker stop $(docker ps -qf \u0026quot;ancestor=hello-go\u0026quot;)\n  Tagging and Pushing to a Container Registry  Tag the new image with tag 0.0.2 using your image/repo/tag\ndocker tag hello-go jabbottc1/hello-go:0.0.2\n Push to a docker registry\n$ docker push jabbottc1/hello-go:0.0.2 The push refers to repository [docker.io/jabbottc1/hello-go] ...  Edit your hello-go-deployment from the Critical Stack UI. Update the image in the Deployment to pull 0.0.2.\n image: 'jabbottc1/hello-go:0.0.2' # Change this tag  Save your deployment and Exit\n Test your upgraded deployment. You can run the same tests as above but targeting the Load Balancer.\n$ curl -s -H \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;name\u0026quot;:\u0026quot;John Test\u0026quot;}' https://\u0026lt;URL_to_your_application\u0026gt;/todos  $ curl -s https://\u0026lt;URL_to_your_application\u0026gt;/todos | python -m json.tool [ { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;Write presentation\u0026quot; }, { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 2, \u0026quot;name\u0026quot;: \u0026quot;Host meetup\u0026quot; }, { \u0026quot;completed\u0026quot;: false, \u0026quot;due\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;id\u0026quot;: 3, \u0026quot;name\u0026quot;: \u0026quot;John Test\u0026quot; } ]   Conclusion You have successfully deployed a RESTful Go application to Critical Stack. But this application is flawed - it can\u0026rsquo;t scale beyond 1 node and has no persistence! So next we will update the application with persistence and scale the deployment to multiple instances.\nNote: some bugs with DELETE!\nTODO Show how to build with scripts\n sh build.sh -v 0.0.2  "
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/go/",
	"title": "Go Labs",
	"tags": [],
	"description": "",
	"content": "  Feature Labs Feature labs illustrated using Go\n"
},
{
	"uri": "https://criticalstack.github.io/labs/administration/kubectl101/",
	"title": "Kubectl 101",
	"tags": [],
	"description": "",
	"content": " Kubectl 101 Getting Started Pre-requisites:\n A Critical Stack cluster. You will need to know the admin username and password.\n kubectl installed : kubectl  Getting Started Source material for this introduction borrowed from kubernetes.io documentation.\nKubectl is a command line interface for running commands against Kubernetes clusters. This overview covers kubectl syntax, describes the command operations, and provides common examples. For details about each command, including all the supported flags and subcommands, see the kubectl reference documentation.\nUse the following syntax to run kubectl commands from your terminal window:\nkubectl [command] [TYPE] [NAME] [flags]  where command, TYPE, NAME, and flags are:\n command Specifies the operation that you want to perform on one or more resources, for example create, get, describe, delete. Type: Specifies the resource type. Some examples are : pods, nodes, services, deployments. Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms. For example, the following commands produce the same output:\nkubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1  NAME: Specifies the name of the resource. Names are case-sensitive. If the name is omitted, details for all resources are displayed.\n flags: Specifies optional flags. For example, you can use the -s or \u0026ndash;server flags to specify the address and port of the Kubernetes API server.\n  Connecting to your server kubectl Config Files A config file, typically stored in ~/.kube/config, defines the cluster, user, and namespace to use for each kubectl command. This guide will walk through creating a new config file for a Critical Stack cluster. If you are already using minikube or another kubernetes cluster, follow this guide for multiple clusters.\nCreate a file called ~/.kube/config, and place this header at the top:\napiVersion: v1 kind: Config preferences: {}  Info about your cluster On a new line, create the tag clusters:, and create a cluster: tag as a child. Name the cluster something familiar, and provide its URL as server: under cluster:. The Secure port is 6443\napiVersion: v1 kind: Config preferences: {} clusters: - cluster: server: https://CS-Cluster-61010-elb-1512390147.us-east-1.elb.amazonaws.com:6443 name: example-cluster  Your login info Create a users: block, and a user: object as a child. Name that user, and provide your authentication information. You\u0026rsquo;ll be authenticating with a token, which you can find in Critical Stack by going to the cluster admin\u0026rsquo;s namespace (top right hand corner) then to \u0026ldquo;Config\u0026rdquo; then \u0026ldquo;Secrets\u0026rdquo;. You\u0026rsquo;ll see a secret named \u0026lt;adminnamespace\u0026gt;-token-\u0026lt;random id\u0026gt;. Don\u0026rsquo;t use the token labeled default-token-\u0026lt;random id\u0026gt;, as this token will not work. Click that token name, and you\u0026rsquo;ll see a clipboard icon to copy token. This is your kubectl authentication token. Copy the ca.crt content and create a new file locally. You will reference the certificate-authority as well.\napiVersion: v1 kind: Config preferences: {} clusters: - cluster: server: https://CS-Cluster-61010-elb-1512390147.us-east-1.elb.amazonaws.com:6443 certificate-authority: /Users/testuser/ca.crt name: example-cluster users: - name: example-user user: token: \u0026lt;your token here\u0026gt;  Using your cluster A context describes how to use your cluster. Create a contexts: block, then a context: as a child. Name the context, and provide the cluster, namespace, and user. Then, set your current-context: to the context you just created.\napiVersion: v1 clusters: - cluster: certificate-authority: /Users/hee617/.ssh/cs-cluster-ca.crt server: https://CS-Cluster-61010-elb-1532390197.us-east-1.elb.amazonaws.com name: cs-cluster contexts: - context: cluster: cs-cluster user: clusteradmin name: cs-cluster current-context: cs-cluster kind: Config preferences: {} users: - name: clusteradmin user: token:   By default kubectl uses the configuration in $HOME/.kube/config. If you want to run commands against multiple servers (CS or minikube) you will need a multi config file or use Environment variables.\n To view your current config you can run this command:\nkubectl config view\n  Basic commands  Lets test our a few commands and compare with what we see in the UI.\nkubectl get pods\n$ kubectl get pods NAME READY STATUS RESTARTS AGE hello-go-deployment-85b5b5cc6f-9h4wh 1/1 Running 0 4d hello-go-deployment-85b5b5cc6f-ccsd5 1/1 Running 0 4d hello-go-deployment-85b5b5cc6f-s6htw 1/1 Running 0 5d   Note you will have have access to the resource types that your have permission to view/create within your namespace. If you are the cluster admin you will have permission to see all namespaces and resources.\n In this example, I have 3 pods running all from one deployment. Let\u0026rsquo;s take a look at the deployment:\nkubectl get deployments\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hello-go-deployment 3 3 3 3 6d   To view the configuration for this deployment, run this command:\nkubectl describe deployment\n$ kubectl describe deployment Name: hello-go-deployment Namespace: \u0026lt;namespace\u0026gt; CreationTimestamp: Wed, 17 Apr 2019 10:18:24 -0400 Labels: app=hello-go Annotations: deployment.kubernetes.io/revision=3 Selector: app=hello-go Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: app=hello-go Containers: hello-go: Image: jabbottc1/hello-go:0.0.3 Port: 8080/TCP  Let\u0026rsquo;s change the replicas value of the deployment. You could do this from the UI, but we will change this from the command line.\nkubectl scale deployment hello-go-deployment --replicas=2\n$ kubectl scale deployment hello-go-deployment --replicas=2 kubectl describe deployment Name: hello-go-deployment Namespace: \u0026lt;namespace\u0026gt; CreationTimestamp: Wed, 17 Apr 2019 10:18:24 -0400 Labels: app=hello-go Annotations: deployment.kubernetes.io/revision=3 Selector: app=hello-go Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: app=hello-go   Conclusion You have successfully setup kubectl with your Critical Stack cluster and can run some basic commands to view and update existing resources.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/featurelabs/node/updating/",
	"title": "Updating a NodeJS App",
	"tags": [],
	"description": "",
	"content": " Using Critical Stack to update a scalable, stateless application Getting Started Before starting on this lab, you will need to have completed the Previous Node lab.\nOverview In the previous lab we created a simple NodeJS application, packaged the application in a docker image, pushed the docker image to a public docker image repository (Docker Hub), pulled that image into a Critical Stack deployment as a container instance, and accessed the application via a public URL.\nIn this next lab, we will create more instances of the container, update the application to show bread crumbs of those instances, push that update to Docker Hub, and use the Critical Stack UI to instruct Kubernetes to roll out the update while we refresh the public URL. This will let us observe that a seamless rolling update is done with zero downtime.\nSteps  Edit app/index.js (in your node-lab directory) and replace the source code with the following:\n// Load the http module to create an http server. var http = require('http'); // Load the os module to access network interfaces. var os = require('os'); // Load the crypto module to generate a random number. var crypto = require(\u0026quot;crypto\u0026quot;); // Get all interfaces by IP address var interfaces = os.networkInterfaces(); var addresses = []; for (var k in interfaces) { for (var k2 in interfaces[k]) { var address = interfaces[k][k2]; if (address.family === 'IPv4' \u0026amp;\u0026amp; !address.internal) { addresses.push(address.address); } } } // Generate a random number var random_number = crypto.randomBytes(16).toString(\u0026quot;hex\u0026quot;); // Configure our HTTP server to respond with Hello World to all requests. var server = http.createServer(function (request, response) { response.writeHead(200, {\u0026quot;Content-Type\u0026quot;: \u0026quot;text/plain\u0026quot;}); response.end(\u0026quot;Hello World from \u0026quot; + addresses + \u0026quot; (\u0026quot; + random_number + \u0026quot;)\\n\u0026quot;); }); // Listen on port 3000 server.listen(3000, \u0026quot;0.0.0.0\u0026quot;); // Put a friendly message on the terminal console.log(\u0026quot;Server IPs: \u0026quot; + addresses); console.log(\u0026quot;Random number: \u0026quot; + random_number); console.log(\u0026quot;Server running at http://0.0.0.0:3000/\u0026quot;);  Increment the application version number in your package.json file and re-run npm install to update the package-lock.json file.\nuser@testhost node-lab$ cat package.json { \u0026quot;name\u0026quot;: \u0026quot;hello-node\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.0.2\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Say hello from Node\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;app/index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Error: no test specified\\\u0026quot; \u0026amp;\u0026amp; exit 1\u0026quot; }, \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot; } user@testhost node-lab$ npm install npm WARN hello-node@0.0.2 No repository field. up to date in 0.632s found 0 vulnerabilities  Build your docker image again, but this time tag it as \u0026lt;your-username\u0026gt;/hello-node:0.0.2 so we can uniquely identify the new image. After it\u0026rsquo;s built, push it to Docker Hub so it can be updated in your Critical Stack cluster.\nuser@testhost node-lab$ docker build -t \u0026lt;your-username\u0026gt;/hello-node:0.0.2 -f Dockerfile . Sending build context to Docker daemon 4.288MB Step 1/9 : FROM node:9 ---\u0026gt; 08a8c8089ab1 Step 2/9 : RUN mkdir /src ---\u0026gt; Using cache ---\u0026gt; 3530d1bbe38a Step 3/9 : WORKDIR /src ---\u0026gt; Using cache ---\u0026gt; 4096063f6a2b Step 4/9 : COPY ./package.json /src/package.json ---\u0026gt; ad23e7eb360e Step 5/9 : COPY ./package-lock.json /src/package-lock.json ---\u0026gt; 46c675f287b6 Step 6/9 : RUN npm install --silent ---\u0026gt; Running in d3144afb3a54 up to date in 0.076s Removing intermediate container d3144afb3a54 ---\u0026gt; 56cbd547eb6d Step 7/9 : COPY ./app /src/app ---\u0026gt; 50b39bb7ea64 Step 8/9 : EXPOSE 3000 ---\u0026gt; Running in 0ab1df6c4e2c Removing intermediate container 0ab1df6c4e2c ---\u0026gt; 67b69d1d4337 Step 9/9 : CMD [\u0026quot;node\u0026quot;, \u0026quot;app/index.js\u0026quot;] ---\u0026gt; Running in 7925bde282fc Removing intermediate container 7925bde282fc ---\u0026gt; be1e663ce0d2 Successfully built be1e663ce0d2 Successfully tagged \u0026lt;your-username\u0026gt;/hello-node:0.0.2 user@testhost node-lab$ docker push \u0026lt;your-username\u0026gt;/hello-node:0.0.2 The push refers to repository [docker.io/\u0026lt;your-username\u0026gt;/hello-node] 13108b60c999: Pushed 328a2707105e: Pushed 700b0faff5ac: Pushed eb80df9a56cf: Pushed df64ff0ed93f: Layer already exists 71521673e105: Layer already exists 7695686f75c0: Layer already exists e492023cc4f9: Layer already exists cbda574aa37a: Layer already exists 8451f9fe0016: Layer already exists 858cd8541f7e: Layer already exists a42d312a03bb: Layer already exists dd1eb1fd7e08: Layer already exists 0.0.2: digest: sha256:2e1002b635cd983e7f2458331079f7b131d8096fd5994287c6f4933438513367 size: 3041  Go to the Critical Stack UI under Deployment Listings, find the row my-first-deployment, click on the gear icon, and select Scale. Move the slider for Number of podS to 2 and then click OK.\n The previous step declaratively set the desired state of our deployment\u0026rsquo;s pods. Watch the deployment scale from 1 to 2:\n Click on the gear icon again, and select Edit. On line 36, change the Docker image tag from 0.0.1 to 0.0.2. For example: image: \u0026lsquo;docker.io/jabbottc1/hello-node:0.0.1\u0026rsquo; becomes image: \u0026lsquo;docker.io/jabbottc1/hello-node:0.0.2\u0026rsquo;. Next click Save and then Exit.\nNote that while we are editing the deployment, another way to scale the application is to directly update the yaml as shown below.\n Refresh your browser pointing to the public URL for your application and watch the update roll out. Also notice the different output when traffic shifts from one pod to another. Note: changing Idle timeout to 1 in the load balancer configuration will reduce the amount of time between switching from one pod to another (in the same section used to get the public facing URL in the previous lab).\n If you want to go back to the previous application version for any reason, simply edit the deployment again and change the Docker image tag from 0.0.2 back to 0.0.1.\n  Conclusion You should now feel comfortable with the basics of pushing new docker images to a repository and pulling them into a Critical Stack deployment running multiple instances of your image. This process is the basis for ongoing updates.\n"
},
{
	"uri": "https://criticalstack.github.io/labs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Critical Stack Labs Self-driven lab activities for getting to know the Critical Stack product\n"
},
{
	"uri": "https://criticalstack.github.io/labs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://criticalstack.github.io/labs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]